# **Data Lake**
This project is designed to build an ETL pipeline for a data lake hosted on S3 for a music streaming startup, Sparkify. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app. 

To complete the project, tasks to be performed are as follows: 

* Create EMR cluster with the specified steps using the AWS Python SDK.
* Extract data from S3.
* Process the data using Spark.
* Loads the data back into S3 as a set of dimensional tables.

## **Data**
Sparkify has been collecting data regarding songs and user activity on the app. Datasets reside in S3. Here are the S3 links for each:

* Song data: s3://udacity-dend/song_data
* Log data: s3://udacity-dend/log_data
  
  * Log data json path: s3://udacity-dend/log_json_path.json

* **Song Dataset**

    Each file is in JSON format and contains metadata about a song and the artist of that song.

        {"num_songs": 1, "artist_id": "ARBGXIG122988F409D", "artist_latitude": 37.77916, "artist_longitude": -122.42005, "artist_location": "California - SF", "artist_name": "Steel Rain", "song_id": "SOOJPRH12A8C141995", "title": "Loaded Like A Gun", "duration": 173.19138, "year": 0}

* **Log Dataset**
    
    Log dataset consists of log files in JSON format generated by [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. The log files in the dataset are partitioned by year and month.

    A log in a log file looks like: 
     
        {"artist":null,"auth":"Logged In","firstName":"Walter","gender":"M","itemInSession":0,"lastName":"Frye","length":null, "level":"free","location":"San Francisco-Oakland-Hayward, CA","method":"GET","page":"Home","registration":1540919166796.0, "sessionId":38,"song":null,"status":200,"ts":1541105830796,"userAgent":"\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"39"} 


## **Database Schema**
Here, Star schema is utilized to speed up read queries by culling out data releated to business and separating it from descriptive data.

* **Fact Table**
    * **songplays** 

        Records in log data associated with song plays

        |       Column        |       Type         |     Description    |
        |:-------------------:|:------------------:|:------------------:|
        | `songplay_id` | SERIAL, PRIMARY KEY | Unique identifier of song plays|
        | `start_time`  | TIMESTAMP, NOT NULL, FOREIGN KEY | Time when song plays happened |
        | `user_id`     | INT, NOT NULL, FOREIGN KEY | Unique identifier of  users |  
        | `level`       | VARCHAR | Level of users: paid or free plan | 
        | `song_id`     | VARCHAR, FOREIGN KEY | Unique identifier of songs | 
        | `artist_id`   | VARCHAR, FOREIGN KEY | Unique identifier of artists |
        | `session_id`  | INT | Unique identifier of sessions | 
        | `location`    | VARCHAR | Location of users | 
        | `user_agent`  | VARCHAR | Agent used to access Sparkify music app |

* **Dimension Tables**
    * **users**

        Users in the app
        |       Column        |       Type         |     Description    |
        |:-------------------:|:------------------:|:------------------:|
        | `user_id` | INT, PRIMARY KEY | Unique identifier of users |
        | `first_name` | VARCHAR | first name of the user |
        | `last_name` | VARCHAR | last name of the user |
        | `gender` | CHAR(1) | gender of the user: F or M |
        | `level` | VARCHAR | Level of the user: paid or free plan |  

    * **songs**

        Songs in the music database

        |       Column        |       Type         |     Description    |
        |:-------------------:|:------------------:|:------------------:|
        | `song_id` | VARCHAR, PRIMARY KEY | Unique identifier of songs |  
        | `title` | VARCHAR | Name of the song | 
        | `artist_id` | VARCHAR, NOT NULL, FOREIGN KEY | Artist id of the song | 
        | `year` | INT | Year when the song was realeased |
        | `duration` | NUMERIC | Duration of the song in ms | 

    * **artists**

        Artists in the music database

        |       Column        |       Type         |     Description    |
        |:-------------------:|:------------------:|:------------------:|
        | `artist_id` | VARCHAR, PRIMARY KEY | Unique identifier of artists | 
        | `name` | VARCHAR | Name of the artist |
        | `location` | VARCHAR | Location of the artist |
        | `latitude` | NUMERIC | Latitude of the location of the artist | 
        | `longitude` | NUMERIC | Longitude of the location of the artist |  

    * **time**

        Timestamps of records in songplays broken down into specific units

        |       Column        |       Type         |     Description    |
        |:-------------------:|:------------------:|:------------------:|        
        | `start_time` | TIMESTAMP, PRIMARY KEY | Time when song plays happened |
        | `hour` | INT, NOT NULL | Hour of the `start_time` | 
        | `day` | INT, NOT NULL | Day of the `start_time` |
        | `week` | INT, NOT NULL | Week of year for `start_time` | 
        | `month` | INT, NOT NULL | Month of `start_time` |
        | `year` | INT, NOT NULL | Year of `start_time` | 
        | `weekday` | INT, NOT NULL | Day of the week of `start_time`: Monday=0, Sunday=6 | 

## **Project Files**
* `emr.py`: creates an S3 bucket, uploads `etl.py` to it, creates an EMR cluster, and adds an step to be run on the cluster.
* `etl.py`: reads data from S3, processes that data using Spark, and writes them back to S3
* `dl.cfg`: contains your AWS credentials

## **How to run**

   1. Fill the following parts in configuration file named **`dl.cfg`**: 
   
    [AWS]
    AWS_ACCESS_KEY_ID=
    AWS_SECRET_ACCESS_KEY=

    [EMR]
    EMR_REGION=
    NUM_NODE= 

    [S3]
    BUCKET_NAME=

   2. Run **`emr.py`** in the console:

    python emr.py

* Notes:
  - EMR cluster status can be checked in AWS Management Console.
  - Output tables are in S3 once the status completed.